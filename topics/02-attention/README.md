# Attention

This unit explains how attention works in neural networks, including self-attention, cross-attention, and common variants. You will learn the math, intuition, and how to implement scaled dot-product attention, multi-head attention, and visualize attention patterns.

Sections:
- Notes under `notes/`
- Code (NumPy/PyTorch) under `code/`
- Visualizations under `visuals/`
- Tiny datasets under `data/`
- Quiz and references
